{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7uxgRHhoyKXR"
   },
   "source": [
    "# tensorflow 2.0: Notebook 2: Computer Vision with CNNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QS8ejf4q1Af6"
   },
   "source": [
    "## 1. Introduction to this Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an-J0lbeYKJx"
   },
   "source": [
    "* In this notebook, we will use the **`mnist`** dataset -  to build on our knowledge. In particular, we will:\n",
    "  * introduce **`Computer Vision`** \n",
    "  * introduce **`convolutional layers`** into our models \n",
    "  * introduce the concept of **`regularisation`**\n",
    "  * introduce how to **`save`** and reuse our model \n",
    "* The image below sets out how this fits within our deep learning framework and exising knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQQ2UeuUnpb3"
   },
   "source": [
    "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%202%20-%20Summary_final.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bE6VXlttqngA"
   },
   "source": [
    "### 1.1  Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAeFpNkuq3Sd"
   },
   "outputs": [],
   "source": [
    "## importing as per previous notebook\n",
    "\n",
    "# We are future proofing by importing modules that modify or replace exising modules that we may have used now \n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# import tensorflow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# import helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's print out the version we are using \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLCYkpZf1-sa"
   },
   "outputs": [],
   "source": [
    "## some additional imports for this notebook \n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uy7-uMr30gxw"
   },
   "source": [
    "### 1.2 Loading our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-i1MWLw2br1"
   },
   "outputs": [],
   "source": [
    "# split our data \n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VozmgZb26Qx"
   },
   "outputs": [],
   "source": [
    "# lets have a quick look at our data \n",
    "plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8m3va3c3oOx"
   },
   "outputs": [],
   "source": [
    "# and at our labels\n",
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9Y29leh3hj5"
   },
   "source": [
    "**What is the problem we are trying to solve?**\n",
    "* As we can see, we have images of digits from 0 - 9, and labels from 0 - 9. We are trying to build a model that correctly classifies the digits in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q2volUNyV8i"
   },
   "source": [
    "## 2. Data: Introduction to Computer Vision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIw5BMMfYGEl"
   },
   "source": [
    "**What is Computer Vision?**\n",
    "* Computer Vision is the field of how computers can gain understanding from images and videos. It includes tasks such as **`image recognition`** and **`object detection`**. Deep Learning is seen as the state of the art technology for solving computer vision problems. \n",
    "\n",
    "**Why is Deep Learning particularly good at it?**\n",
    "* The layers within a deep learning model are good for identifying and modelling the different aspects of an image (such as edges, parts of faces, and other important parts of an image). The meaning that each layer extracts can be built up to form representations for lots of different image types that can then be classified.  \n",
    "* In particular, **`convolutional layers`** are good at extracting representation from image data and they form the basis of deep learning models for image recognition. The ability to build larger and larger models that consist of these convolutional layers, and to train them with more and more data (thanks to increasing compute power), led to a leap forward in state of the art for computer vision. \n",
    "\n",
    "**How does it work?**\n",
    "* Every image is represented by an array of numbers. You may have noticed that the **`shape`** of the images we are processing. This shape represents the number of pixels in an image, and each pixel has a numerical value. This numerical value maps to a colour value that is displayed. It is also what we use as input values to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os-EKjfMLa9K"
   },
   "outputs": [],
   "source": [
    "## lets start by looking at the shape of an image\n",
    "\n",
    "## we can see that it is 28 x 28 pixels\n",
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "if3gXQQbu8JV"
   },
   "outputs": [],
   "source": [
    "## we can also see that these pixels are represented in an array of numbers \n",
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AOm9r_-5N6f"
   },
   "outputs": [],
   "source": [
    "# we need plt.imshow() - or another library such as OpenCV or PIL - to output an image from this array\n",
    "plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDMsDeU_zACw"
   },
   "source": [
    "**What do the array values mean?**\n",
    "* Each value leads to a colour for the pixel that the array value represents. Actualy what colour is displayed depends somewhat on the number of colour channels the array has. We have only one channel present in this dataset. This is grayscale channel. Typically, we will see three channels for colour images, with each channel representing one of Red, Green, Blue. A value in one channel will display a different colour than a value in another channel. \n",
    "* Its worth noting here that there are typically 256 values (0 - 255) available in each channel, making a total combination of c.\n",
    "* We will rescale the arrays to between 0 and 1. This needs to happen in order to maximise the success of the training. \n",
    "\n",
    "**What about images of different shapes?**\n",
    "* The size of an image can and does vary. In this case, we have small image of 28 x 28 pixels (or 28, 28, 1) given we have one channel.\n",
    "* One final thing to note is that Deep Learning models always require an array of the same size to be passed to it. This means that images which differ in size need to be preprocessed so that they are the same size before being passed to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAxv79drLfaJ"
   },
   "outputs": [],
   "source": [
    "# we now need to reshape the data to add a colour channel \n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pt4aS5nn3mpL"
   },
   "outputs": [],
   "source": [
    "# we can view the new shape \n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klwaHSVyL4jw"
   },
   "outputs": [],
   "source": [
    "# and normalize the data \n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuwQA6fTUKca"
   },
   "source": [
    "**So far pointers considered**\n",
    "* Images have to be fed into the model in the same shape each time. This requires pre-processing.\n",
    "* Prior to feed images into a model, we can also change the image in certain ways to add noise and variety to the training data. This should mean that the model is more robust and better at generalizing to unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr6NFGzV33c7"
   },
   "source": [
    "## 3. Model Building "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in1xg1Eh-iF_"
   },
   "source": [
    "### 3.1 Convolutional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plOECBsyXtCl"
   },
   "source": [
    "To built the model we will take an image as an input and flattened it to a **1D** array. \n",
    "\n",
    "**How do we build a convolutional neural network?**\n",
    "* A convolutional neural network (CNN) contains both dense and convolutional layers. The convolutional layers form the **`base`** of the model and extracts representation from the image. The dense layers form the **`head`** of the model and takes this represetation and maps it to our output classes\n",
    "* A convolutional layer takes our image as it (subject to any preprocessing to get it in a standard shape or augmented to add noise and variety to the dataset) - that is, we do not need to flatten the image into a **`1D`** array. We flatten the array after our final convolutional layer and prior to passing our input to the dense layer.\n",
    "\n",
    "**Why use a convolutional layer?**\n",
    "* A convolution better encodes the key information in an image than other types of layers. Their application to computer vision resulted in a marked improvement in what was state of the art. That's why we use them. \n",
    "\n",
    "**What is a convolutional layer?**\n",
    "* Simply, a convolutional layer is a layer that performs a mathematical operations known as convolutional on the input data.\n",
    "* Each convolutional layer have a user-defined set of filters (or windows) that we pass over the image. We define the number and size of filters, although they are typically a 3 x 3 matrix. \n",
    "* This filter contains a set of weights that will be learned by the model and which are used to multiply the input values and return a new value in the layer's output. Its these filters that contain the learning of the convolutional layers of the model, whose weights will be updated as we train so that they are more and more able to extract key information from the image.\n",
    "* The filter is applied to all the image channels as it passes over each pixel location such that it will look at a specific row and column index position and all the array values available at that index:\n",
    "$$(row, column, :)$$ \n",
    "\n",
    "**So what does a convolutional layer return?**\n",
    "* A convolutional layer returns an output array of the same (row, column) shape as the input array, but with one channel only. \n",
    "* It tends to be the case that convolutional layer is paired with a **`pooling layer`**, a pooling layer tries to extract the key information from the convolutional layer while typically halving its size. \n",
    "* The diagram below set this out. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuADaW8uNb-D"
   },
   "source": [
    "![alt text](https://github.com/DanRHowarth/Tensorflow-2.0/blob/master/Notebook%202%20Convolution%20and%20Pooling%20Layers.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7oAv7C13_eT"
   },
   "outputs": [],
   "source": [
    "## lets build our convolutional base\n",
    "## we use the Sequential API but use .add() rather than passing the layers in as a list\n",
    "\n",
    "# build model using sequential \n",
    "model = models.Sequential() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMCb5IdENjxL"
   },
   "outputs": [],
   "source": [
    "# start adding layers. input shape has been defined, including the channel value we added via reshape earlier\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "# max pooling layers\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "# this is then repeated to build \n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# max pooling layers\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "# additional convolutional layer \n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhezngEi5yQY"
   },
   "outputs": [],
   "source": [
    "# print model \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7VAnSx54L-B"
   },
   "source": [
    "**How do we get to the parameter count?**\n",
    "* The parameters of a convolutional layer are defined by:\n",
    "$$((filter\\ height\\ \\times filter\\ width)\\ +\\ bias\\ term)\\ \\times number\\ of\\ filters$$\n",
    "\n",
    "* The bias term is a value of 1 so the number of parameters for the first convolutional layer is:\n",
    "$$((3 \\times 3)+ 1)\\ \\times 32 = 320$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfRNOIjiUigH"
   },
   "source": [
    "**What about the classificaiton layer?**\n",
    "* As we said above, a convolutional based needs a classification layer to take the information extracted from an image and map it to output classes. \n",
    "* We take the final output shape of the Convolutional layer and flatten it to a 1D array. We then define our output layer, which in this instance is a layer of 10 with a softmax activation function. We have also added an additional layer to provide additional parameters between the flattened and output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKQpmbk-UjUo"
   },
   "outputs": [],
   "source": [
    "# flatten \n",
    "model.add(layers.Flatten())\n",
    "# add a fully connected layer\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# add the output layer, a fully connected layer with a softmax activation\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcUQrBgtUmEK"
   },
   "outputs": [],
   "source": [
    "# complete model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-wZF5jB6YMu"
   },
   "source": [
    "## 4. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSEAp6bmIS9L"
   },
   "source": [
    "### 4.1 Validation Sets, Batch Sizes and Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIm3yY_VKMBa"
   },
   "source": [
    "* A validation set is a dataset that is kept aside from the training dataset. Tpyically, at the end of each epoch, the trained model is passed the validation set in inference model, and the loss and other metrics recorded. The model is then trained again for another epoch, and at the end of this epoch is passed the validation set in inference mode. \n",
    "* This allows us to see during training how the model performs on unseen data and also whether the model is **`under- or over-fitting`**. \n",
    "\n",
    "**What is under and overfitting?**\n",
    "* When we train our models, they can sometimes struggle to generalise well. This means that they do not perform well on unseen data. \n",
    "* A model that overfits will get better and better results (loss and metrics) on the training data and decreasing results on the validation set. This is because it is fitting too much to the specific characteristics of the training data, which may not be present in the unseen data.\n",
    "* A model that underfits does not perform well on either the training or validation data. \n",
    "* By incluidng a validation set,  we can monitor how well the model performs, and if the performance on the training and validation sets diverge too much then we can start to conclude that something need to be changed. \n",
    "\n",
    "**What can we do to address this?**\n",
    "* We can use **`regularization`** to help prevent overfitting.\n",
    "* We can regularize our model and its training in a number of ways, and to some extent they all penalize actions that may cause the model to overfit to the trianing data:\n",
    "  * When building a model, we add a **`dropout`** layer. This turns off a user defined number of outputs from a layer r during training and helps prevents the model becoming reliant on certain paths through the model.\n",
    "  *  During training, **`weight regularization`**. This penalises large weights (our learned parameters) in the model. Larger weights will create a larger loss value that the mdoel will then use to update the weights. A model with fewer larger weights, i.e. with the learning more evenly spread across the nodes, will have a better chance of generalizing well.   \n",
    "\n",
    "**What is a batch_size?**\n",
    "* A batch size is the number of samples the model trains on before performing a backward pass (model update). The number of batches in an epoch is equal to:\n",
    "$$ \\frac{number\\ of\\ sample\\ in\\ a\\ training\\ set}{batch\\ size}$$\n",
    "* We can set the batch size and choose to see the model performance as trains per batch size (using a parameter in **`model.fit()`**). \n",
    "\n",
    "**What is the learning rate?**\n",
    "* The learning rate controls the size of the update to the learnable parameters (weights and biases) during the backward pass, based on the loss of the model.\n",
    "* It is a parameter of the optimizer, set at the **`model.compile()`** stage, and can be set by the user. The trick is to set the learning to update the weights sufficiently to change performance, but not update them too much so that the model weights swing between higher and lower values without settling on a path to the best model.\n",
    "* There are various strategies for mitigating this problem that are worth investigating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXRPgaFK6dWI"
   },
   "outputs": [],
   "source": [
    "## Lets compile the model again\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaEy1MqF6rh4"
   },
   "outputs": [],
   "source": [
    "## and build a training loop\n",
    "\n",
    "history = model.fit(train_images, train_labels, \n",
    "          # add batch size\n",
    "          batch_size = 32,\n",
    "          # epochs           \n",
    "          epochs = 10,\n",
    "          # and add a validation set \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzsUx60J7TxW"
   },
   "source": [
    "**Did we do any good?**\n",
    "* 98% plus on the validation set seems pretty good!\n",
    "* Let's plot the loss and accuracy and see what it shows us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw5Gcq1ZIdd3"
   },
   "outputs": [],
   "source": [
    "# if we print out our history object we can see that it now includes validation values\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLvVXg9m7W75"
   },
   "outputs": [],
   "source": [
    "## we can pass this to a pandas dataframe\n",
    "\n",
    "# we need to import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# pass history data to dataframe object\n",
    "history_df = pd.DataFrame(history_dict)\n",
    "\n",
    "# and display it \n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bR21IEmy7cVe"
   },
   "outputs": [],
   "source": [
    "# we can use plot functionality of pandas to quickly plot our results\n",
    "history_df.plot(figsize=(8,5))\n",
    "# tailor our plot. Show the grid\n",
    "plt.grid(True)\n",
    "# set the vertical range to [0 -1]\n",
    "plt.gca().set_ylim(0, 1)\n",
    "# display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZNzgPdpP1Wq"
   },
   "source": [
    "### 4.4 Saving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocpFaHT_2wO8"
   },
   "source": [
    "**What do we mean by *saving a model*?**\n",
    "* We can save our progress as we train our models. We can save our progress in two ways:\n",
    "  * *during training*, so that our models are saved after each epoch (or, after an epoch that shows model improvement). \n",
    "  * *after training*, so that our model has completed its training before we save it\n",
    "* Of course, we can opt to save the model both during *and* after training. \n",
    "* Using `tensorflow 2.0`, we can opt to save the model either manually, i.e. after the model has trained, or by using callbacks - i.e. incorporating saving into the training process. \n",
    "  \n",
    "**What are we saving?**\n",
    "* We can save the model weights only, the full model (including the weights and the architecture), and the optimizer state.\n",
    "* Its useful to remember that when we are training a model, the parameters we are updating during the training process are the weights at each layer of the model. Our aim is to train on (data, labels) pairs that mean we can predict effectively on unseen data using the weights we have trained. So it is these weights that are saved, optionally along with the model architecture. \n",
    "* Optimizer state. We haven't focussed too much on optimizers, but remember that this is the way that the model weights are updated. The size of the update is set by the (user defined) **`learning rate`**. When we save a model we can therefore save the **`optimizer-state`**, meaning we can continue training a loaded model from the state it was in when the model was saved.  \n",
    "\n",
    "**Why save a model?**\n",
    "* So that we can reuse it later. This could be to deploy it and use it in inference mode, or to continue training from the point at which we stopped. \n",
    "\n",
    "**Once we have saved a model, how do we use it again?**\n",
    "* Once we have saved our weights and/or model, we can restore the model in a couple of different ways. If we decide to save the weights only, we need to create an identical model to the one that was used to create our weights. If we saved both the model and weights, we can load this entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OsFddwQCC6M"
   },
   "source": [
    "#### 4.3.1 Saving and Loading Weights Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQcYM1kpJJRM"
   },
   "outputs": [],
   "source": [
    "## lets go through the steps of saving the model weights only\n",
    "\n",
    "# here we define a location for the weights to be saved \n",
    "model.save_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HotnbUgC1ntp"
   },
   "outputs": [],
   "source": [
    "# to load the weights we need to create a new instance of the same model architecture \n",
    "new_instance = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKXouUmZJKpO"
   },
   "outputs": [],
   "source": [
    "# then we can load the weights \n",
    "new_instance.load_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYQWFY-d1onp"
   },
   "outputs": [],
   "source": [
    "new_instance.weights[0][0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYCpb0ddCHVP"
   },
   "source": [
    "#### 4.3.2 Saving and Loading an entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3RO-cUlfCKmj"
   },
   "outputs": [],
   "source": [
    "# this saves it to the HDF5 format\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDAwnX8-DGZi"
   },
   "outputs": [],
   "source": [
    "# recreate the saved model, including weights and optimizer \n",
    "new_model = keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTs_ZeQPwkQH"
   },
   "outputs": [],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sms9gfHEVa1W"
   },
   "source": [
    "###So, what did we cover in this section?\n",
    "* Validation sets\n",
    "* Overfitting \n",
    "* Regularization\n",
    "* Learning Rates, Batch Sizes\n",
    "* Saving Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqT0LadcXjut"
   },
   "source": [
    "## 5. Evaluation and Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-DNj1kKP5yo"
   },
   "outputs": [],
   "source": [
    "## using the model we have just saved, evaluate it on the test set \n",
    "## if you are stuck, use some of the code from notebook 1\n",
    "# \n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "## if we had saved our weights or model and loaded them up, we would use the model\n",
    "## variable name we had saved here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IMEKcUrrwVH"
   },
   "source": [
    "## 6. Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpQhedM8rznH"
   },
   "source": [
    "* Work through Chapter 15 - page 517 - of Python Machine Learning for another implementation of a Convolutional Neural Network\n",
    "* Optionally, work through Chapter 14 for a deeper understanding of tensorflow 2.0\n",
    "* There are a lot of good articles out there too. For example, [Cezanne Comacho](https://cezannec.github.io/Convolutional_Neural_Networks/), and [Chris Olah](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/) all provide a good understanding of how convolution works.\n",
    "* For the maths of convolution, have a look at [this paper](https://arxiv.org/pdf/1603.07285.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "QS8ejf4q1Af6",
    "4q2volUNyV8i",
    "yr6NFGzV33c7",
    "w-wZF5jB6YMu",
    "KqT0LadcXjut",
    "_KPv43nP_Qan",
    "hOQN6p4zF5t_",
    "7IMEKcUrrwVH"
   ],
   "name": "tf2_Notebook 2_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
