{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFQ8UWLBEBx4"
   },
   "source": [
    "## Cat vs. Dog\n",
    "\n",
    "### Transfer Learning\n",
    "\n",
    "In this notebook, you will learn to use pretrained network VGG-16 to extract image features from the cats and dog images. Then we will use a simple Multilayer perceptron to classify the images using the above extracted features as inputs.\n",
    "\n",
    "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, We can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmnFW2aHEBx7"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R66tfjfyEBx7"
   },
   "source": [
    "The recommended folder structure is:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyJDOoeJEBx7"
   },
   "source": [
    "### Folder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h77K-Z2aEBx8"
   },
   "source": [
    "```python\n",
    "dataset/\n",
    "    train/ ###8000\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/ \n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    test/ ###2000\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMal06X5EBx8"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3mk2wp-PEBx9"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiPumsIREBx-"
   },
   "source": [
    "ImageDataGenerator method in keras to feed all the images into model. ImageDataGenerator will automatically label the data and map all the labels to its specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "et-iYGd6EBx-",
    "outputId": "8cac7c15-20b6-427a-dea6-542048979680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator()\n",
    "traindata = trdata.flow_from_directory(directory=r\"dataset/train\",target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nZ-U0JfWEBx_",
    "outputId": "44d6afe9-a34b-40c0-833d-142f459b4143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "tsdata = ImageDataGenerator()\n",
    "testdata = tsdata.flow_from_directory(directory=r\"dataset/test\", target_size=(224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1OAJdpcEBx_"
   },
   "source": [
    "## Using a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUhpVjKdEBx_"
   },
   "source": [
    "The process of training a convolutionnal neural network can be very time-consuming and require a lot of datas.  \n",
    "\n",
    "In this tutorial we'll use VGG16, a model trained on the ImageNet dataset - which contains millions of images classified in 1000 categories. \n",
    "\n",
    "On top of it, we add a small multi-layer perceptron and we train it on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGv2I6wbEByA"
   },
   "source": [
    "## VGG-16 Network\n",
    "\n",
    "The VGG-16 is a 16-layer network used by the VGG team in the ILSVRC-2014 competition. The network architecture and implementation details can be found in this [paper](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "Though the VGG-16 model did not win the ILSVRC competition, the simplicity of the sequential model has led to it being used for a lot of transfer learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFpcLoWhEByA"
   },
   "source": [
    "Here in this part we'll import VGG16 from keras with pre-trained weights which was trained on imagenet. Here as you can see that include top parameter is set to true. This means that weights for our whole model will be downloaded. If this is set to false then the pre-trained weights will only be downloaded for convolution layers and no weights will be downloaded for dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hgUN7PJHEByA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 10:43:33.890025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-05 10:43:33.959612: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-12-05 10:43:33.959646: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-12-05 10:43:33.960747: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 58s 0us/step\n",
      "553476096/553467096 [==============================] - 58s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "vggmodel = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "W_glMtc2EByB",
    "outputId": "a6bffdba-b073-440a-f086-504d8ebdd0c1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vggmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6EXm6feEByB"
   },
   "source": [
    "here will set trainable parameter to False for first 19 layers because we'll not be training the weights of the first 19 layers and use it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fIhqCZSHEByC",
    "outputId": "edadf39a-1520-468b-eb98-f0a9c4b529b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7f0834430130>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d1dd040>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d1dd8b0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f079d1dd820>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d128100>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d12d9d0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f079d16ec70>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d131370>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d138d00>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d0c0f10>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f079d12dc10>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d0cb070>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d0d10d0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d0d15b0>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f079d131f40>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d0db3d0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d0e04f0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f079d0db160>\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f079d0e01c0>\n"
     ]
    }
   ],
   "source": [
    "for layers in (vggmodel.layers)[:19]:\n",
    "    print(layers)\n",
    "    layers.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUb7Gk7uEByC"
   },
   "source": [
    "Since our problem is to detect cats and dogs and it has two classes so the last dense layer of model should have 2 unit softmax dense layer. Also taking the second last layer of the model which is dense layer with 4096 units and adding a dense softmax layer of 2 units in the end to remove the last layer of the VGG16 model which is made to predict 1000 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aLInxwjHEByC"
   },
   "outputs": [],
   "source": [
    "X= vggmodel.layers[-2].output\n",
    "predictions = Dense(2, activation=\"softmax\")(X)\n",
    "model_final = Model(vggmodel.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "f64j1c9VEByD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paresh/.cache/pypoetry/virtualenvs/upgrad-masters-ml-ai-7jz2oC-U-py3.8/lib/python3.8/site-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cWDw7omfEByD",
    "outputId": "44fec4d8-0877-441f-d3aa-805b56ce91b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 8194      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 119,554,050\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZAC-RKaEByD"
   },
   "source": [
    "ModelCheckpoint helps us to save the model by monitoring a specific parameter of the model. In this case we are monitoring validation accuracy by passing val_accuracy to ModelCheckpoint. The model will only be saved to disk if the validation accuracy of the model in current epoch is greater than what it was in the last epoch.\n",
    "\n",
    "EarlyStopping helps us to stop the training of the model early if there is no increase in the parameter which we have set to monitor in EarlyStopping. In this case we are monitoring validation accuracy by passing val_accuracy to EarlyStopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UTmra4ZDEByD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "early = EarlyStopping(monitor='val_accuracy', patience=3, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ipValKXEByE"
   },
   "source": [
    "Here we'll pass train and test data to fit_generator. In fit_generator steps_per_epoch will set the batch size to pass training data to the model and validation_steps will do the same for test data. tweak it based on your system specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oC4B7Z4rEByE",
    "outputId": "0ed970e6-3d19-4746-fbf0-f44154ea99ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16826/2316152776.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model_final.fit_generator(generator= traindata, steps_per_epoch= 2, epochs= 5, validation_data= testdata, validation_steps=1, callbacks=[checkpoint,early])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 [==============================] - ETA: 0s - loss: 1.1170 - accuracy: 0.6562\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.75000, saving model to model.h5\n",
      "2/2 [==============================] - 25s 19s/step - loss: 1.1170 - accuracy: 0.6562 - val_loss: 0.4728 - val_accuracy: 0.7500\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7489 - accuracy: 0.7031\n",
      "Epoch 00002: val_accuracy improved from 0.75000 to 0.78125, saving model to model.h5\n",
      "2/2 [==============================] - 25s 20s/step - loss: 0.7489 - accuracy: 0.7031 - val_loss: 0.4852 - val_accuracy: 0.7812\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2992 - accuracy: 0.8438\n",
      "Epoch 00003: val_accuracy improved from 0.78125 to 0.87500, saving model to model.h5\n",
      "2/2 [==============================] - 25s 19s/step - loss: 0.2992 - accuracy: 0.8438 - val_loss: 0.2958 - val_accuracy: 0.8750\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4294 - accuracy: 0.8438\n",
      "Epoch 00004: val_accuracy improved from 0.87500 to 0.90625, saving model to model.h5\n",
      "2/2 [==============================] - 27s 22s/step - loss: 0.4294 - accuracy: 0.8438 - val_loss: 0.4241 - val_accuracy: 0.9062\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.2259 - accuracy: 0.9219\n",
      "Epoch 00005: val_accuracy improved from 0.90625 to 0.96875, saving model to model.h5\n",
      "2/2 [==============================] - 29s 23s/step - loss: 0.2259 - accuracy: 0.9219 - val_loss: 0.0670 - val_accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f079d1285e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.fit_generator(generator= traindata, steps_per_epoch= 2, epochs= 5, validation_data= testdata, validation_steps=1, callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZCHUqkRTEByE",
    "outputId": "d08dbd8d-b888-42f5-c2e4-f3fe942b13b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16826/2484837891.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  test_loss, test_acc = model_final.evaluate_generator(testdata, steps=2)\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_final.evaluate_generator(testdata, steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "A_00obcKEByE",
    "outputId": "d08b8169-a3fb-4e64-8427-01872d660b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 0.890625\n",
      "test loss: 0.22538495063781738\n"
     ]
    }
   ],
   "source": [
    "print('test acc:', test_acc)\n",
    "print('test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwwfPU3xEByF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "cat_vs_dog vgg16.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
